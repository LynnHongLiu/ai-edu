Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 15.6 批量归一化的实现



### 15.5.5 反向传播

首先假设已知从上一层回传给BN层的误差矩阵是：

$$\delta = {dJ \over dZ}，\delta_i = {dJ \over dz_i} \tag{10}$$

#### 求BN层参数梯度

则根据公式9，求$\gamma 和 \beta$的梯度：

$${dJ \over d\gamma} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\gamma}=\sum_{i=1}^m \delta_i \cdot n_i \tag{11}$$

$${dJ \over d\beta} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\beta}=\sum_{i=1}^m \delta_i \tag{12}$$

注意$\gamma和\beta$的形状与批大小无关，只与特征值数量有关，我们假设特征值数量为1，所以它们都是一个标量。在从计算图看，它们都与N,Z的全集相关，而不是某一个样本，因此会用求和方式计算。

#### 求BN层的前传误差矩阵

下述所有乘法都是element-wise的矩阵点乘，不再特殊说明。

从正向公式中看，对z有贡献的数据链是：

- $z_i \leftarrow n_i \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \mu_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow \mu_B \leftarrow x_i$

从公式8，9：

$$
{dJ \over dx_i} = {dJ \over d n_i}{d n_i \over dx_i} + {dJ \over d \sigma^2_B}{d \sigma^2_B \over dx_i} + {dJ \over d \mu_B}{d \mu_B \over dx_i} \tag{13}
$$

公式13的右侧第一部分（与全连接层形式一样）：

$$
{dJ \over d n_i}=  {dJ \over dz_i}{dz_i \over dn_i} = \delta_i \cdot \gamma\tag{14}
$$

上式等价于：

$$
{dJ \over d N}= \delta \cdot \gamma\tag{14}
$$

公式14中，我们假设样本数为64，特征值数为10，则得到一个64x10的结果矩阵（因为1x10的矩阵会被广播为64x10的矩阵）：

$$\delta^{(64 \times 10)} \odot \gamma^{(1 \times 10)}=R^{(64 \times 10)}$$

公式13的右侧第二部分，从公式8：
$$
{d n_i \over dx_i}={1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{15}
$$

公式13的右侧第三部分，从公式8（注意$\sigma^2_B$是个标量，而且与X,N的全集相关，要用求和方式）：

$$
{dJ \over d \sigma^2_B} = \sum_{i=1}^m {dJ \over d n_i}{d n_i \over d \sigma^2_B} 
$$
$$
= -{1 \over 2}(\sigma^2_B + \epsilon)^{-3/2}\sum_{i=1}^m {dJ \over d n_i} \cdot (x_i-\mu_B) \tag{16}
$$


公式13的右侧第四部分，从公式7：
$$
{d \sigma^2_B \over dx_i} = {2(x_i - \mu_B) \over m} \tag{17}
$$

公式13的右侧第五部分，从公式7，8：

$$
{dJ \over d \mu_B}={dJ \over d n_i}{d n_i \over d \mu_B} + {dJ \over  d\sigma^2_B}{d \sigma^2_B \over d \mu_B} \tag{18}
$$

公式18的右侧第二部分，根据公式8：

$$
{d n_i \over d \mu_B}={-1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{19}
$$

公式18的右侧第四部分，根据公式7（$\sigma^2_B和\mu_B$与全体$x_i$相关，所以要用求和）：

$$
{d \sigma^2_B \over d \mu_B}=-{2 \over m}\sum_{i=1}^m (x_i- \mu_B) \tag{20}
$$

所以公式18是：

$$
{dJ \over d \mu_B}=-{\delta \cdot \gamma \over \sqrt{\sigma^2_B + \epsilon}} - {2 \over m}{dJ \over d \sigma^2_B}\sum_{i=1}^m (x_i- \mu_B) \tag{18}
$$

公式13的右侧第六部分，从公式6：

$$
{d \mu_B \over dx_i} = {1 \over m} \tag{21}
$$

所以，公式13最后是这样的：

$$
{dJ \over dx_i} = {\delta \cdot \gamma \over \sqrt{\sigma^2_B + \epsilon}} + {dJ \over d\sigma^2_B} \cdot {2(x_i - \mu_B) \over m} + {dJ \over d\mu_B} \cdot {1 \over m} \tag{13}
$$


### 参考资料

https://arxiv.org/abs/1502.03167


